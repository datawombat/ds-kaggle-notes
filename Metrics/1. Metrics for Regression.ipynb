{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Bigger font\n",
    "# sns.set_context(\"poster\")\n",
    "sns.set_context(\"talk\")\n",
    "# Figure size\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 4\n",
    "# np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Things to consider regarding the metric:\n",
    "\n",
    "- Optimize the model for the metric.\n",
    "- There are tricks to boost the score after the modeling by using the metric for the competition.\n",
    "\n",
    "### Constant Model\n",
    "\n",
    "When the prediction is a constant.\n",
    "\n",
    "This **optimal constant** is key when it comes to taking advantage of the metric.\n",
    "\n",
    "The error for this constant model is called **constant baseline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a. MSE, RMSE, and $R^2$\n",
    "\n",
    "- In general, it is easier to work with MSE than with RMSE.\n",
    "- But they can be interchangeable.\n",
    "\n",
    "#### The gradient\n",
    "- For gradient-based models, as a loss function, the gradient of RMSE is a little more complex, so MSE is preferred.\n",
    "\n",
    "MSE is the most common metric. It's best value is 0, and there is one that simply its square root: RMSE.\n",
    "\n",
    "#### Why the square root?\n",
    "\n",
    "The square root is introduced **to make the scale of the error to be the same as the scale of the target**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "0.6123724356957945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(mean_squared_error(y_true, y_pred))\n",
    "print(np.sqrt(mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sensitive to outliers\n",
    "\n",
    "MSE and RMSE are very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9801.375\n",
      "99.0018939212781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 200, 8]\n",
    "print(mean_squared_error(y_true, y_pred))\n",
    "print(np.sqrt(mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### R-squared or $R^2$\n",
    "\n",
    "R-squared values range from 0 to 1, and it measures how close the data are to the fitted regression line. The closer to 1, the better.\n",
    "\n",
    "- When MSE = 0, $R^2$ = 1.\n",
    "- When computing the MSE of a constant model (the average), then $R^2$ = 0.\n",
    "\n",
    "This is a better way to evaluate models.\n",
    "\n",
    "- $R^2$ is the normalized version of MSE.\n",
    "- People use MSE for reporting because it is the loss-function models minimize.\n",
    "- $R^2$ is useful because it is often easier to interpret since it doesn't depend on the scale of the data. So $R^2$ would make it easier to state which model is performing better.\n",
    "\n",
    "#### Optimal constant = Mean\n",
    "\n",
    "The constant prediction that makes and optimal MSE is the average \n",
    "$y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.296875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "## CONSTANT MODEL\n",
    "prediction = np.mean(y_true)\n",
    "y_pred = np.ones(4)*prediction\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## b. MAE\n",
    "\n",
    "- Less sentitive to outliers\n",
    "- It is widely used in finance where $\\$10$ error is exactly two times worse than $\\$5$ error.\n",
    "- MAE is easier to justify.\n",
    "\n",
    "#### The gradient\n",
    "\n",
    "Its derivative is the step function.\n",
    "\n",
    "The gradient is not defined when the prediction is perfect, so formally MAE is not differentiable, but there's a simple \"if\" workaround when $y_i = \\hat{y}_i$\n",
    "\n",
    "The second derivative is non-defined as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimal constant = Median\n",
    "\n",
    "It is the median of the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "## CONSTANT MODEL\n",
    "prediction = np.median(y_true)\n",
    "y_pred = np.ones(4)*prediction\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do you have outliers (which are not values that should be ignored) in the data? Use MAE.\n",
    "- Are they just unexpected values we should care about? Use MSE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## c. Mean Square Percentage Error ((R)MSPE)\n",
    "\n",
    "If $y = 10$ and $\\hat{y} = 9$, then MSE = 1  \n",
    "If $y = 1000$ and $\\hat{y} = 999$, then MSE = 1\n",
    "\n",
    "Probably, we consider that difference of 1 more significant in the first case because it's a smaller value. Then MSE or MAE wouldn't be a good metric because **MSE works with absolute errors**.\n",
    "\n",
    "#### Relative error\n",
    "\n",
    "If $y = 10$ and $\\hat{y} = 9$, then relative_error = 1  \n",
    "If $y = 1000$ and $\\hat{y} = 900$, then relative_error = 1\n",
    "\n",
    "![](images/errors.png)\n",
    "- As the target increases, the cost becomes less\n",
    "- (R)MSPE and MAPE are weighted versions.\n",
    "\n",
    "#### Optimal constant = Weighted mean of the target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## d. Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "The same as with (R)MSPE, but with absolute values.\n",
    "\n",
    "#### Optimal constant = Weighted median of the target vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## e. Root mean square logarithmic error ((R)MSLE)\n",
    "\n",
    "A constant \"1\" is added because log(0) is not defined.\n",
    "\n",
    "Used for the same situation as MSPE and MAPE.\n",
    "\n",
    "![](images/rmsle.png)\n",
    "#### Optimal constant = It is the mean target value exponentiated"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
